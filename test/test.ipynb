{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Dict\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "from luxai_s3.wrappers import LuxAIS3GymEnv\n",
    "import gymnasium as gym\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3 import PPO\n",
    "import numpy as np\n",
    "from typing import Optional, Dict, Any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LuxAIWrapper(gym.Wrapper):\n",
    "    def __init__(self, env: LuxAIS3GymEnv, player_id: int = 0):\n",
    "        \"\"\"\n",
    "        A comprehensive wrapper for the LuxAI environment that handles both:\n",
    "        1. Stable Baselines compatibility\n",
    "        2. Reward calculations\n",
    "        \n",
    "        Args:\n",
    "            env: The base LuxAI environment\n",
    "            player_id: Which player this agent controls (0 or 1)\n",
    "        \"\"\"\n",
    "        super().__init__(env)\n",
    "        self.player_id = player_id\n",
    "        \n",
    "        # Set up state tracking for dense rewards\n",
    "        self.previous_total_energy = None\n",
    "        self.previous_relic_points = None\n",
    "        self.previous_unit_count = None\n",
    "        \n",
    "        # Set up the observation and action spaces\n",
    "        self._setup_spaces()\n",
    "\n",
    "\n",
    "    def _setup_spaces(self):\n",
    "        \"\"\"\n",
    "        Configures the observation and action spaces for compatibility with \n",
    "        Stable Baselines. We use Box spaces for both to ensure compatibility\n",
    "        with standard neural network policies.\n",
    "        \"\"\"\n",
    "        # Action space configuration:\n",
    "        # - Each unit (16 total) needs 3 values:\n",
    "        #   1. Action type (0-5)\n",
    "        #   2. Sap x coordinate (-4 to 4)\n",
    "        #   3. Sap y coordinate (-4 to 4)\n",
    "        self.action_space = gym.spaces.Box(\n",
    "            low=np.array([0, -4, -4] * 16),\n",
    "            high=np.array([5, 4, 4] * 16),\n",
    "            dtype=np.int32\n",
    "        )\n",
    "\n",
    "        # Observation space configuration:\n",
    "        # Using a flat vector representation of the game state\n",
    "        total_obs_size = 1862  # Total size of flattened observation\n",
    "        self.observation_space = gym.spaces.Box(\n",
    "            low=-float('inf'),\n",
    "            high=float('inf'),\n",
    "            shape=(total_obs_size,),\n",
    "            dtype=np.float32\n",
    "        )\n",
    "\n",
    "\n",
    "    def calculate_reward(self, obs : Dict[str, Any]) -> float:\n",
    "        \"\"\"\n",
    "        Calculates a reward signal based on multiple\n",
    "        gameplay factors.\n",
    "        \n",
    "        Args:\n",
    "            obs: Dictionary containing the current observation\n",
    "            \n",
    "        Returns:\n",
    "            float: The caluclated reward\n",
    "        \"\"\"\n",
    "        current_reward = 0.0\n",
    "        # TODO: Maybe extract more from environment\n",
    "        # Extract current state information\n",
    "        unit_energies = obs['units']['energy'].reshape(-1)\n",
    "        unit_mask = obs['units_mask'].reshape(-1)\n",
    "        team_points = obs['team_points']\n",
    "        \n",
    "        # Calculate current metrics\n",
    "        total_energy = np.sum(unit_energies * unit_mask)\n",
    "        current_unit_count = np.sum(unit_mask)\n",
    "        current_relic_points = team_points[self.player_id]\n",
    "        # TODO: Need to tinker with these rewards\n",
    "        # --Energy management reward--\n",
    "        # Reward for maintaining and increasing total energy across all units\n",
    "        if self.previous_total_energy is not None:\n",
    "            energy_diff = total_energy - self.previous_total_energy\n",
    "            current_reward += 0.001 * energy_diff\n",
    "        \n",
    "        # --Unit survival reward--\n",
    "        # Significant reward for maintaining and growing the unit count\n",
    "        if self.previous_unit_count is not None:\n",
    "            unit_diff = current_unit_count - self.previous_unit_count\n",
    "            current_reward += 0.1 * unit_diff\n",
    "        \n",
    "        # --Relic control reward--\n",
    "        # Major reward for controlling relic points as it's a key victory condition\n",
    "        if self.previous_relic_points is not None:\n",
    "            points_diff = current_relic_points - self.previous_relic_points\n",
    "            current_reward += 0.2 * points_diff\n",
    "        \n",
    "        # --Unit health reward--\n",
    "        # Small reward for maintaining healthy units\n",
    "        alive_units = unit_energies[unit_mask > 0]\n",
    "        if len(alive_units) > 0:\n",
    "            avg_unit_energy = np.mean(alive_units)\n",
    "            current_reward += 0.0005 * avg_unit_energy\n",
    "                \n",
    "        # Update state tracking for next step\n",
    "        self.previous_total_energy = total_energy\n",
    "        self.previous_unit_count = current_unit_count\n",
    "        self.previous_relic_points = current_relic_points\n",
    "        \n",
    "        return current_reward\n",
    "\n",
    "    def _flatten_observation(self, obs:  Dict[str, Any]) -> Any:\n",
    "        \"\"\"\n",
    "        Converts the dictionary observation into a flat array for the neural network.\n",
    "        This standardizes the input format for the learning algorithm.\n",
    "        \"\"\"\n",
    "        components = []\n",
    "        \n",
    "        # Process unit information\n",
    "        unit_positions = obs['units']['position'].reshape(-1)\n",
    "        unit_energies = obs['units']['energy'].reshape(-1)\n",
    "        components.extend([\n",
    "            unit_positions.astype(np.float32),\n",
    "            unit_energies.astype(np.float32)\n",
    "        ])\n",
    "        \n",
    "        # Process map information\n",
    "        units_mask = obs['units_mask'].reshape(-1)\n",
    "        sensor_mask = obs['sensor_mask'].reshape(-1)\n",
    "        map_energy = obs['map_features']['energy'].reshape(-1)\n",
    "        map_tiles = obs['map_features']['tile_type'].reshape(-1)\n",
    "        components.extend([\n",
    "            units_mask.astype(np.float32),\n",
    "            sensor_mask.astype(np.float32),\n",
    "            map_energy.astype(np.float32),\n",
    "            map_tiles.astype(np.float32)\n",
    "        ])\n",
    "        \n",
    "        # Process game state information\n",
    "        components.extend([\n",
    "            obs['team_points'].astype(np.float32),\n",
    "            obs['team_wins'].astype(np.float32),\n",
    "            np.array([obs['steps']], dtype=np.float32),\n",
    "            np.array([obs['match_steps']], dtype=np.float32)\n",
    "        ])\n",
    "        \n",
    "        return np.concatenate(components)\n",
    "\n",
    "    def _unflatten_observation(self, obs) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Converts a flat observation back into dictionary format for reward calculation.\n",
    "        \"\"\"\n",
    "        map_size = 24\n",
    "        max_units = 16\n",
    "        idx = 0\n",
    "        \n",
    "        obs_dict = {}\n",
    "        \n",
    "        # Reconstruct unit information\n",
    "        obs_dict['units'] = {\n",
    "            'position': obs[idx:idx + max_units * 2].reshape(max_units, 2),\n",
    "            'energy': obs[idx + max_units * 2:idx + max_units * 3].reshape(max_units, 1)\n",
    "        }\n",
    "        idx += max_units * 3\n",
    "        \n",
    "        # Reconstruct map information\n",
    "        obs_dict['units_mask'] = obs[idx:idx + max_units].reshape(max_units)\n",
    "        idx += max_units\n",
    "        \n",
    "        obs_dict['sensor_mask'] = obs[idx:idx + map_size * map_size].reshape(map_size, map_size)\n",
    "        idx += map_size * map_size\n",
    "        \n",
    "        obs_dict['map_features'] = {\n",
    "            'energy': obs[idx:idx + map_size * map_size].reshape(map_size, map_size),\n",
    "            'tile_type': obs[idx + map_size * map_size:idx + 2 * map_size * map_size].reshape(map_size, map_size)\n",
    "        }\n",
    "        idx += 2 * map_size * map_size\n",
    "        \n",
    "        # Reconstruct game state information\n",
    "        obs_dict['team_points'] = obs[idx:idx + 2]\n",
    "        idx += 2\n",
    "        obs_dict['team_wins'] = obs[idx:idx + 2]\n",
    "        idx += 2\n",
    "        obs_dict['steps'] = obs[idx]\n",
    "        obs_dict['match_steps'] = obs[idx + 1]\n",
    "        \n",
    "        return obs_dict\n",
    "\n",
    "    def step(self, action: Dict[str, list])->tuple:\n",
    "        \"\"\"\n",
    "        Take a step in the environment with the provided action and return the updated state.\n",
    "\n",
    "        Args:\n",
    "            action (Dict[str, list]): The action to be taken by the player.\n",
    "        Returns:\n",
    "            tuple: A tuple containing the following:\n",
    "                - obs (np.ndarray): The flattened observation for the current player after the step.\n",
    "                - total_reward (float): The total reward for the current player, combining environment and additional rewards.\n",
    "                - terminated (bool): Whether the episode has ended for the current player.\n",
    "                - truncated (bool): Whether the episode was truncated for the current player.\n",
    "                - info (dict): Additional information from the environment.\n",
    "        \"\"\"\n",
    "\n",
    "       \n",
    "        # Convert and reshape action for the environment\n",
    "        shaped_action = {\n",
    "            f'player_{self.player_id}': action.reshape(16, 3).astype(np.int32),\n",
    "            f'player_{1-self.player_id}': np.zeros((16, 3), dtype=np.int32)\n",
    "        }\n",
    "        \n",
    "        # Take step in environment\n",
    "        obs, rewards, terminated, truncated, info = self.env.step(shaped_action)\n",
    "        \n",
    "        # Get observation for our player\n",
    "        player_obs = obs[f'player_{self.player_id}']\n",
    "        \n",
    "        # Calculate reward\n",
    "        original_reward = rewards[f'player_{self.player_id}']\n",
    "        added_reward = self.calculate_reward(player_obs)\n",
    "        \n",
    "        # Combine rewards\n",
    "        total_reward = original_reward + added_reward\n",
    "\n",
    "        # Convert observation to flat format\n",
    "        obs = self._flatten_observation(player_obs)\n",
    "        \n",
    "        return obs, total_reward, terminated[f'player_{self.player_id}'], truncated[f'player_{self.player_id}'], info\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    def reset(self, *, seed: Optional[int] = None, options: Optional[Dict[str, Any]] = None)-> tuple:\n",
    "        \"\"\"\n",
    "        Resets the environment and initializes tracking variables for rewards.\n",
    "\n",
    "        Args:\n",
    "            seed (Optional[int]): An optional seed for resetting the environment's random state.\n",
    "            options (Optional[Dict[str, Any]]): Optional dictionary containing additional reset parameters.\n",
    "\n",
    "        Returns:\n",
    "            tuple: A tuple containing:\n",
    "                - dict: The observation for the specified player.\n",
    "                - dict: Additional information about the environment after reset.\n",
    "\n",
    "        \"\"\"\n",
    "        # Reset reward tracking\n",
    "        self.previous_total_energy = None\n",
    "        self.previous_relic_points = None\n",
    "        self.previous_unit_count = None\n",
    "        \n",
    "        # Reset environment\n",
    "        obs, info = self.env.reset(seed=seed, options=options)\n",
    "        player_obs = obs[f'player_{self.player_id}']\n",
    "        obs = self._flatten_observation(player_obs)\n",
    "        \n",
    "        return obs, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 206  |\n",
      "|    iterations      | 1    |\n",
      "|    time_elapsed    | 19   |\n",
      "|    total_timesteps | 4096 |\n",
      "-----------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 197         |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 41          |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015079102 |\n",
      "|    clip_fraction        | 0.177       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -68.1       |\n",
      "|    explained_variance   | 0.018       |\n",
      "|    learning_rate        | 3e-05       |\n",
      "|    loss                 | 42.2        |\n",
      "|    n_updates            | 15          |\n",
      "|    policy_gradient_loss | -0.0564     |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 107         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 195         |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 62          |\n",
      "|    total_timesteps      | 12288       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010549739 |\n",
      "|    clip_fraction        | 0.101       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -68.1       |\n",
      "|    explained_variance   | 0.225       |\n",
      "|    learning_rate        | 3e-05       |\n",
      "|    loss                 | 199         |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0369     |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 402         |\n",
      "-----------------------------------------\n",
      "---------------------------------------\n",
      "| time/                   |           |\n",
      "|    fps                  | 193       |\n",
      "|    iterations           | 4         |\n",
      "|    time_elapsed         | 84        |\n",
      "|    total_timesteps      | 16384     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0136112 |\n",
      "|    clip_fraction        | 0.141     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -68.1     |\n",
      "|    explained_variance   | 0.249     |\n",
      "|    learning_rate        | 3e-05     |\n",
      "|    loss                 | 152       |\n",
      "|    n_updates            | 45        |\n",
      "|    policy_gradient_loss | -0.0447   |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 394       |\n",
      "---------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 186         |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 109         |\n",
      "|    total_timesteps      | 20480       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015503474 |\n",
      "|    clip_fraction        | 0.187       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -68.1       |\n",
      "|    explained_variance   | 0.447       |\n",
      "|    learning_rate        | 3e-05       |\n",
      "|    loss                 | 55.7        |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.0572     |\n",
      "|    std                  | 0.999       |\n",
      "|    value_loss           | 137         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 183         |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 134         |\n",
      "|    total_timesteps      | 24576       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017221123 |\n",
      "|    clip_fraction        | 0.2         |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -68.1       |\n",
      "|    explained_variance   | 0.308       |\n",
      "|    learning_rate        | 3e-05       |\n",
      "|    loss                 | 117         |\n",
      "|    n_updates            | 75          |\n",
      "|    policy_gradient_loss | -0.0602     |\n",
      "|    std                  | 0.999       |\n",
      "|    value_loss           | 223         |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 26\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Create and train model with additional parameters to handle discrete actions\u001b[39;00m\n\u001b[0;32m      7\u001b[0m model \u001b[38;5;241m=\u001b[39m PPO(\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMlpPolicy\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      9\u001b[0m     vec_env,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     22\u001b[0m     device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m  \u001b[38;5;66;03m# Use CPU as recommended for MLP\u001b[39;00m\n\u001b[0;32m     23\u001b[0m )\n\u001b[1;32m---> 26\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000000\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\tobia\\anaconda3\\envs\\neurIPS\\Lib\\site-packages\\stable_baselines3\\ppo\\ppo.py:311\u001b[0m, in \u001b[0;36mPPO.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    302\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlearn\u001b[39m(\n\u001b[0;32m    303\u001b[0m     \u001b[38;5;28mself\u001b[39m: SelfPPO,\n\u001b[0;32m    304\u001b[0m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    309\u001b[0m     progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    310\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SelfPPO:\n\u001b[1;32m--> 311\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    312\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    313\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    314\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    315\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    316\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    317\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    318\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\tobia\\anaconda3\\envs\\neurIPS\\Lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:323\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    320\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    322\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps \u001b[38;5;241m<\u001b[39m total_timesteps:\n\u001b[1;32m--> 323\u001b[0m     continue_training \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect_rollouts\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrollout_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_rollout_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_steps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    325\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m continue_training:\n\u001b[0;32m    326\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\tobia\\anaconda3\\envs\\neurIPS\\Lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:218\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.collect_rollouts\u001b[1;34m(self, env, callback, rollout_buffer, n_rollout_steps)\u001b[0m\n\u001b[0;32m    213\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    214\u001b[0m         \u001b[38;5;66;03m# Otherwise, clip the actions to avoid out of bound error\u001b[39;00m\n\u001b[0;32m    215\u001b[0m         \u001b[38;5;66;03m# as we are sampling from an unbounded Gaussian distribution\u001b[39;00m\n\u001b[0;32m    216\u001b[0m         clipped_actions \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mclip(actions, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mlow, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mhigh)\n\u001b[1;32m--> 218\u001b[0m new_obs, rewards, dones, infos \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclipped_actions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    220\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mnum_envs\n\u001b[0;32m    222\u001b[0m \u001b[38;5;66;03m# Give access to local variables\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\tobia\\anaconda3\\envs\\neurIPS\\Lib\\site-packages\\stable_baselines3\\common\\vec_env\\base_vec_env.py:206\u001b[0m, in \u001b[0;36mVecEnv.step\u001b[1;34m(self, actions)\u001b[0m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    200\u001b[0m \u001b[38;5;124;03mStep the environments with the given action\u001b[39;00m\n\u001b[0;32m    201\u001b[0m \n\u001b[0;32m    202\u001b[0m \u001b[38;5;124;03m:param actions: the action\u001b[39;00m\n\u001b[0;32m    203\u001b[0m \u001b[38;5;124;03m:return: observation, reward, done, information\u001b[39;00m\n\u001b[0;32m    204\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    205\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_async(actions)\n\u001b[1;32m--> 206\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\tobia\\anaconda3\\envs\\neurIPS\\Lib\\site-packages\\stable_baselines3\\common\\vec_env\\dummy_vec_env.py:72\u001b[0m, in \u001b[0;36mDummyVecEnv.step_wait\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     70\u001b[0m         obs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreset_infos[env_idx] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menvs[env_idx]\u001b[38;5;241m.\u001b[39mreset()\n\u001b[0;32m     71\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_save_obs(env_idx, obs)\n\u001b[1;32m---> 72\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_obs_from_buf(), np\u001b[38;5;241m.\u001b[39mcopy(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_rews), np\u001b[38;5;241m.\u001b[39mcopy(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_dones), \u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuf_infos\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32mc:\\Users\\tobia\\anaconda3\\envs\\neurIPS\\Lib\\copy.py:136\u001b[0m, in \u001b[0;36mdeepcopy\u001b[1;34m(x, memo, _nil)\u001b[0m\n\u001b[0;32m    134\u001b[0m copier \u001b[38;5;241m=\u001b[39m _deepcopy_dispatch\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;28mcls\u001b[39m)\n\u001b[0;32m    135\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m copier \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 136\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[43mcopier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    137\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    138\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(\u001b[38;5;28mcls\u001b[39m, \u001b[38;5;28mtype\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\tobia\\anaconda3\\envs\\neurIPS\\Lib\\copy.py:196\u001b[0m, in \u001b[0;36m_deepcopy_list\u001b[1;34m(x, memo, deepcopy)\u001b[0m\n\u001b[0;32m    194\u001b[0m append \u001b[38;5;241m=\u001b[39m y\u001b[38;5;241m.\u001b[39mappend\n\u001b[0;32m    195\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m x:\n\u001b[1;32m--> 196\u001b[0m     append(\u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    197\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m y\n",
      "File \u001b[1;32mc:\\Users\\tobia\\anaconda3\\envs\\neurIPS\\Lib\\copy.py:136\u001b[0m, in \u001b[0;36mdeepcopy\u001b[1;34m(x, memo, _nil)\u001b[0m\n\u001b[0;32m    134\u001b[0m copier \u001b[38;5;241m=\u001b[39m _deepcopy_dispatch\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;28mcls\u001b[39m)\n\u001b[0;32m    135\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m copier \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 136\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[43mcopier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    137\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    138\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(\u001b[38;5;28mcls\u001b[39m, \u001b[38;5;28mtype\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\tobia\\anaconda3\\envs\\neurIPS\\Lib\\copy.py:221\u001b[0m, in \u001b[0;36m_deepcopy_dict\u001b[1;34m(x, memo, deepcopy)\u001b[0m\n\u001b[0;32m    219\u001b[0m memo[\u001b[38;5;28mid\u001b[39m(x)] \u001b[38;5;241m=\u001b[39m y\n\u001b[0;32m    220\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m x\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m--> 221\u001b[0m     y[deepcopy(key, memo)] \u001b[38;5;241m=\u001b[39m \u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    222\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m y\n",
      "File \u001b[1;32mc:\\Users\\tobia\\anaconda3\\envs\\neurIPS\\Lib\\copy.py:136\u001b[0m, in \u001b[0;36mdeepcopy\u001b[1;34m(x, memo, _nil)\u001b[0m\n\u001b[0;32m    134\u001b[0m copier \u001b[38;5;241m=\u001b[39m _deepcopy_dispatch\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;28mcls\u001b[39m)\n\u001b[0;32m    135\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m copier \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 136\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[43mcopier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    137\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    138\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(\u001b[38;5;28mcls\u001b[39m, \u001b[38;5;28mtype\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\tobia\\anaconda3\\envs\\neurIPS\\Lib\\copy.py:221\u001b[0m, in \u001b[0;36m_deepcopy_dict\u001b[1;34m(x, memo, deepcopy)\u001b[0m\n\u001b[0;32m    219\u001b[0m memo[\u001b[38;5;28mid\u001b[39m(x)] \u001b[38;5;241m=\u001b[39m y\n\u001b[0;32m    220\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m x\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m--> 221\u001b[0m     y[deepcopy(key, memo)] \u001b[38;5;241m=\u001b[39m \u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    222\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m y\n",
      "File \u001b[1;32mc:\\Users\\tobia\\anaconda3\\envs\\neurIPS\\Lib\\copy.py:162\u001b[0m, in \u001b[0;36mdeepcopy\u001b[1;34m(x, memo, _nil)\u001b[0m\n\u001b[0;32m    160\u001b[0m                 y \u001b[38;5;241m=\u001b[39m x\n\u001b[0;32m    161\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 162\u001b[0m                 y \u001b[38;5;241m=\u001b[39m \u001b[43m_reconstruct\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mrv\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    164\u001b[0m \u001b[38;5;66;03m# If is its own copy, don't memoize.\u001b[39;00m\n\u001b[0;32m    165\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m x:\n",
      "File \u001b[1;32mc:\\Users\\tobia\\anaconda3\\envs\\neurIPS\\Lib\\copy.py:259\u001b[0m, in \u001b[0;36m_reconstruct\u001b[1;34m(x, memo, func, args, state, listiter, dictiter, deepcopy)\u001b[0m\n\u001b[0;32m    257\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m state \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    258\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m deep:\n\u001b[1;32m--> 259\u001b[0m         state \u001b[38;5;241m=\u001b[39m \u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    260\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(y, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__setstate__\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m    261\u001b[0m         y\u001b[38;5;241m.\u001b[39m__setstate__(state)\n",
      "File \u001b[1;32mc:\\Users\\tobia\\anaconda3\\envs\\neurIPS\\Lib\\copy.py:136\u001b[0m, in \u001b[0;36mdeepcopy\u001b[1;34m(x, memo, _nil)\u001b[0m\n\u001b[0;32m    134\u001b[0m copier \u001b[38;5;241m=\u001b[39m _deepcopy_dispatch\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;28mcls\u001b[39m)\n\u001b[0;32m    135\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m copier \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 136\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[43mcopier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    137\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    138\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(\u001b[38;5;28mcls\u001b[39m, \u001b[38;5;28mtype\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\tobia\\anaconda3\\envs\\neurIPS\\Lib\\copy.py:221\u001b[0m, in \u001b[0;36m_deepcopy_dict\u001b[1;34m(x, memo, deepcopy)\u001b[0m\n\u001b[0;32m    219\u001b[0m memo[\u001b[38;5;28mid\u001b[39m(x)] \u001b[38;5;241m=\u001b[39m y\n\u001b[0;32m    220\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m x\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m--> 221\u001b[0m     y[deepcopy(key, memo)] \u001b[38;5;241m=\u001b[39m \u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    222\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m y\n",
      "File \u001b[1;32mc:\\Users\\tobia\\anaconda3\\envs\\neurIPS\\Lib\\copy.py:143\u001b[0m, in \u001b[0;36mdeepcopy\u001b[1;34m(x, memo, _nil)\u001b[0m\n\u001b[0;32m    141\u001b[0m copier \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(x, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__deepcopy__\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    142\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m copier \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 143\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[43mcopier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    144\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    145\u001b[0m     reductor \u001b[38;5;241m=\u001b[39m dispatch_table\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;28mcls\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\tobia\\anaconda3\\envs\\neurIPS\\Lib\\site-packages\\jax\\_src\\numpy\\array_methods.py:595\u001b[0m, in \u001b[0;36m_deepcopy\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m    593\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_deepcopy\u001b[39m(\u001b[38;5;28mself\u001b[39m: Array, memo: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Array:\n\u001b[0;32m    594\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m memo  \u001b[38;5;66;03m# unused\u001b[39;00m\n\u001b[1;32m--> 595\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\tobia\\anaconda3\\envs\\neurIPS\\Lib\\site-packages\\jax\\_src\\numpy\\array_methods.py:163\u001b[0m, in \u001b[0;36m_copy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    158\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_copy\u001b[39m(\u001b[38;5;28mself\u001b[39m: Array) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Array:\n\u001b[0;32m    159\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Return a copy of the array.\u001b[39;00m\n\u001b[0;32m    160\u001b[0m \n\u001b[0;32m    161\u001b[0m \u001b[38;5;124;03m  Refer to :func:`jax.numpy.copy` for the full documentation.\u001b[39;00m\n\u001b[0;32m    162\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[1;32m--> 163\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlax_numpy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\tobia\\anaconda3\\envs\\neurIPS\\Lib\\site-packages\\jax\\_src\\numpy\\lax_numpy.py:5870\u001b[0m, in \u001b[0;36mcopy\u001b[1;34m(a, order)\u001b[0m\n\u001b[0;32m   5826\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Return a copy of the array.\u001b[39;00m\n\u001b[0;32m   5827\u001b[0m \n\u001b[0;32m   5828\u001b[0m \u001b[38;5;124;03mJAX implementation of :func:`numpy.copy`.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   5867\u001b[0m \u001b[38;5;124;03m  [0 1 2 3]\u001b[39;00m\n\u001b[0;32m   5868\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   5869\u001b[0m util\u001b[38;5;241m.\u001b[39mcheck_arraylike(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcopy\u001b[39m\u001b[38;5;124m\"\u001b[39m, a)\n\u001b[1;32m-> 5870\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\tobia\\anaconda3\\envs\\neurIPS\\Lib\\site-packages\\jax\\_src\\numpy\\lax_numpy.py:5641\u001b[0m, in \u001b[0;36marray\u001b[1;34m(object, dtype, copy, order, ndmin, device)\u001b[0m\n\u001b[0;32m   5639\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mobject\u001b[39m, Array):\n\u001b[0;32m   5640\u001b[0m   \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mobject\u001b[39m\u001b[38;5;241m.\u001b[39maval \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 5641\u001b[0m   out \u001b[38;5;241m=\u001b[39m \u001b[43m_array_copy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mobject\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m copy \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mobject\u001b[39m\n\u001b[0;32m   5642\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mobject\u001b[39m, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[0;32m   5643\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mobject\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\tobia\\anaconda3\\envs\\neurIPS\\Lib\\site-packages\\jax\\_src\\lax\\lax.py:6036\u001b[0m, in \u001b[0;36m_array_copy\u001b[1;34m(arr)\u001b[0m\n\u001b[0;32m   6035\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_array_copy\u001b[39m(arr: ArrayLike) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Array:\n\u001b[1;32m-> 6036\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcopy_p\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbind\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\tobia\\anaconda3\\envs\\neurIPS\\Lib\\site-packages\\jax\\_src\\core.py:463\u001b[0m, in \u001b[0;36mPrimitive.bind\u001b[1;34m(self, *args, **params)\u001b[0m\n\u001b[0;32m    461\u001b[0m trace_ctx\u001b[38;5;241m.\u001b[39mset_trace(eval_trace)\n\u001b[0;32m    462\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 463\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbind_with_trace\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprev_trace\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    464\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    465\u001b[0m   trace_ctx\u001b[38;5;241m.\u001b[39mset_trace(prev_trace)\n",
      "File \u001b[1;32mc:\\Users\\tobia\\anaconda3\\envs\\neurIPS\\Lib\\site-packages\\jax\\_src\\core.py:468\u001b[0m, in \u001b[0;36mPrimitive.bind_with_trace\u001b[1;34m(self, trace, args, params)\u001b[0m\n\u001b[0;32m    467\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbind_with_trace\u001b[39m(\u001b[38;5;28mself\u001b[39m, trace, args, params):\n\u001b[1;32m--> 468\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrace\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess_primitive\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\tobia\\anaconda3\\envs\\neurIPS\\Lib\\site-packages\\jax\\_src\\core.py:941\u001b[0m, in \u001b[0;36mEvalTrace.process_primitive\u001b[1;34m(self, primitive, args, params)\u001b[0m\n\u001b[0;32m    939\u001b[0m       \u001b[38;5;28;01mreturn\u001b[39;00m primitive\u001b[38;5;241m.\u001b[39mbind_with_trace(arg\u001b[38;5;241m.\u001b[39m_trace, args, params)\n\u001b[0;32m    940\u001b[0m check_eval_args(args)\n\u001b[1;32m--> 941\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mprimitive\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimpl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\tobia\\anaconda3\\envs\\neurIPS\\Lib\\site-packages\\jax\\_src\\lax\\lax.py:6077\u001b[0m, in \u001b[0;36m_copy_impl\u001b[1;34m(prim, *args, **kwargs)\u001b[0m\n\u001b[0;32m   6075\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m dispatch\u001b[38;5;241m.\u001b[39mapply_primitive(prim, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   6076\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m _copy_impl_pmap_sharding(sharded_dim, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m-> 6077\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdispatch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_primitive\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\tobia\\anaconda3\\envs\\neurIPS\\Lib\\site-packages\\jax\\_src\\dispatch.py:90\u001b[0m, in \u001b[0;36mapply_primitive\u001b[1;34m(prim, *args, **params)\u001b[0m\n\u001b[0;32m     88\u001b[0m prev \u001b[38;5;241m=\u001b[39m lib\u001b[38;5;241m.\u001b[39mjax_jit\u001b[38;5;241m.\u001b[39mswap_thread_local_state_disable_jit(\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     89\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 90\u001b[0m   outs \u001b[38;5;241m=\u001b[39m \u001b[43mfun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     92\u001b[0m   lib\u001b[38;5;241m.\u001b[39mjax_jit\u001b[38;5;241m.\u001b[39mswap_thread_local_state_disable_jit(prev)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Create environment\n",
    "base_env = LuxAIS3GymEnv(numpy_output=True)\n",
    "wrapped_env = LuxAIWrapper(base_env, player_id=0)\n",
    "vec_env = DummyVecEnv([lambda: wrapped_env])\n",
    "\n",
    "# Create and train model with additional parameters to handle discrete actions\n",
    "model = PPO(\n",
    "    \"MlpPolicy\",\n",
    "    vec_env,\n",
    "    verbose=1,\n",
    "    learning_rate=3e-4, \n",
    "    n_steps=4096,\n",
    "    batch_size=128,\n",
    "    n_epochs=15,\n",
    "    gamma=0.99,\n",
    "    policy_kwargs=dict(\n",
    "        net_arch=dict(\n",
    "            pi=[256, 256],\n",
    "            vf=[256, 256]\n",
    "        )\n",
    "    ),\n",
    "    device='cpu'  # Use CPU as recommended for MLP\n",
    ")\n",
    "\n",
    "\n",
    "model.learn(total_timesteps=1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved successfully!\n"
     ]
    }
   ],
   "source": [
    "# Save the model\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "try:\n",
    "    model.save(\"lux_ai_model\")\n",
    "    print(\"Model saved successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"Error saving model: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/william/miniconda3/envs/envneur/lib/python3.10/site-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean reward: 324.20 +/- 247.31\n"
     ]
    }
   ],
   "source": [
    "mean_reward, std_reward = evaluate_policy(model, vec_env, n_eval_episodes=10, deterministic=True)\n",
    "\n",
    "# Print results\n",
    "print(f\"Mean reward: {mean_reward:.2f} +/- {std_reward:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neurIPS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
